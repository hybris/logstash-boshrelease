input {
  redis {
    type => "syslog"
    host => "<%= p("redis.host") %>"
    port => <%= p("redis.port") %>

    # these settings should match the output of the agent
    data_type => "list"
    key => "<%= p('logstash.redis.key') %>"
  }
}


filter {
  if [@type] in ["syslog", "relp"] and [@source.host] == "loggregator" {
    # Parse Cloud Foundry logs from loggregator (syslog)
    # see https://github.com/cloudfoundry/loggregator/blob/master/src/loggregator/sinks/syslogwriter/syslog_writer.go#L156

    grok {
      match => { "syslog_message" => "\[(?<log_source>[^/\]]+)(?:/(?<log_source_id>[^\]]+))?\] \- \-(?: %{GREEDYDATA:message})?" }
        tag_on_failure => [
        "_grokparsefailure-cf-loggregator"
        ]
      }

      if !("_grokparsefailure-cf-loggregator" in [tags]) {
        if [message] =~ /^\s*{".*}\s*$/ {
          mutate {
            rename => [ "message", "_message_json" ]
          }

          json {
            source => "_message_json"
          }

          # @todo seems like some messages have @timestamp in them? seems ci-specific
          date {
            match => [ "@timestamp", "ISO8601" ]
          }

          mutate {
            remove_field => [ "_message_json" ]
          }
        }

        mutate {
          rename => [ "syslog_program", "@source.app_id" ]
        }

        mutate {
          add_tag => "cloudfoundry_loggregator"
          remove_field => "syslog_facility"
          remove_field => "syslog_facility_code"
          remove_field => "syslog_message"
          remove_field => "syslog_severity"
          remove_field => "syslog_severity_code"
          remove_field => "syslog5424_ver"
          remove_field => "syslog6587_msglen"
        }
      }

    } else if [@type] in ["syslog", "relp"] and [syslog_program] =~ /vcap\..*/ {
      # Parse Cloud Foundry logs from syslog_aggregator

      grok {
        match => { "syslog_message" => "(?:\[job=%{NOTSPACE:@job.name}|-) +(?:index=%{NOTSPACE:@job.index}\]|-) %{GREEDYDATA:_message_json}" }
        tag_on_failure => [
        "_grokparsefailure-cf-vcap"
        ]
      }

      if !("_grokparsefailure-cf-vcap" in [tags]) {
        kv {
          source => "msgdata"
          field_split => " "
          target => "msgdata"
        }

        json {
          source => "_message_json"
          remove_field => "_message_json"
        }

        mutate {
          rename => [ "syslog_program", "@shipper.name" ]
          replace => [ "@job.host", "%{@source.host}" ]
          gsub => [
          "@shipper.name", "\.", "_",
          "@job.name", "\.", "_"
          ]
        }

        if [source] == "NatsStreamForwarder" {
          json {
            source => "[data][nats_message]"
            target => "nats_message"
          }

          mutate {
            remove_field => "[data][nats_message]"
          }
        }

        mutate {
          add_tag => "cloudfoundry_vcap"
          replace => [ "@shipper.priority", "%{syslog_pri}" ]
          replace => [ "@shipper.name", "%{@shipper.name}_%{@type}" ]
          replace => [ "@type", "%{@type}_cf" ]
        }

        mutate {
          remove_field => "syslog_facility"
          remove_field => "syslog_facility_code"
          remove_field => "syslog_message"
          remove_field => "syslog_severity"
          remove_field => "syslog_severity_code"
        }
      }

    }

}


output {

  elasticsearch {
    host => "<%= p("elasticsearch.host") %>"
    index => "<%= p("logstash.index") %>-%{+YYYY.MM.dd}"
    protocol => "transport"
    idle_flush_time => <%= p("logstash.idle_flush_time") %>
    workers => <%= p("logstash.workers") %>
    cluster => "<%= p("elasticsearch.cluster") %>"
  }
}
