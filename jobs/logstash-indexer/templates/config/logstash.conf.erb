input {
<% if p("logstash.redis.host", nil) %>
  redis {
    host        => "<%= p("logstash.redis.host") %>"
    port        => <%= p("logstash.redis.port") %>
    batch_count => <%= p('logstash.redis.batch_count') %>
    threads     => <%= p('logstash.redis.threads') %>
    key         => "<%= p('logstash.redis.key') %>"
    data_type   => "list"
  }
<% else %>
  stdin { }
<% end %>
}


filter {
  ###################################
  #              COMMON             #
  ###################################
  if [type] in ["syslog", "relp"] {

    grok {
      match           => { "message" => "(?:%{INT:syslog6587_msglen} )?<%%{POSINT:syslog_pri}>(?:%{NONNEGINT:syslog5424_ver} )?(?:%{SYSLOGTIMESTAMP:syslog_timestamp}|%{TIMESTAMP_ISO8601:syslog_timestamp}) %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?(:)? %{GREEDYDATA:syslog_message}" }
      add_field       => [ "received_at", "%{@timestamp}" ]
      add_field       => [ "received_from", "%{host}" ]
      tag_on_failure  => ["_grokparsefailure-syslog_standard"]
    }

    if !("_grokparsefailure-syslog_standard" in [tags]) {
      syslog_pri { }

      date {
        match     => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss", "ISO8601" ]
        timezone  => "UTC"
      }

      # hostname: handle syslog configurations where hostname is localhost
      if ([syslog_hostname] == "localhost" ) {
        grok {
          match           => { "received_from" => "%{IPORHOST:syslog_hostname}(?::%{POSINT:syslog_port})?" }
          overwrite       => [ "syslog_hostname", "syslog_port" ]
          tag_on_failure  => [ "_grokparsefailure-syslog_standard-hostname"]
        }
      }

      mutate { replace => [ "source.host", "%{syslog_hostname}" ] }
      mutate { convert => [ "syslog5424_ver", "integer" ] }
      mutate { convert => [ "syslog6587_msglen", "integer" ] }

      if [syslog5424_ver] == 1 {
        grok {
          # I don't think this is rfc5424-legal because it says SD *must* exist and message *may* exist.
          # However, this makes parsing compatible with common syslog implementations.
          match           => [ "syslog_message", "(?:%{DATA:syslog_procid}|\-) (?:%{DATA:syslog_msgid}|\-)(?: %{SYSLOG5424SD:syslog_sd}| \-)? %{GREEDYDATA:syslog_message}" ]
          overwrite       => [ "syslog_message" ]
          tag_on_failure  => [ "_grokparsefailure-syslog_standard-5424" ]
        }

        # structured-data
        if [syslog_sd] {
          grok {
            match           => [ "syslog_sd", "\[%{DATA:syslog_sd_id} (?<syslog_sd_params_raw]>[^\]]+)\]" ]
            tag_on_failure  => [ "_grokparsefailure-syslog_standard-5424/sds" ]
          }

          if !("_grokparsefailure-syslog_standard-5424/sd" in [tags]) {
            # convert the the key-value pairs
            kv {
              source        => "syslog_sd_params_raw"
              target        => "syslog_sd_params"
            }

            if [syslog_sd_params][type] {
              # establish a convention that a structured data key of "type" will be the log type
              mutate { replace => { "type" => "%{syslog_sd_params[type]}" } }
            }
          }
        }
      }
    }

    ###################################
    #          NXLOG/BACKING          #
    ###################################
    if "NXLOG@14506" == [syslog_sd_id] {
      # We're going to treat syslog as a transparent transport and pretend it was never even
      # utilized. This means we'll drop the syslog metadata, but it really shouldn't matter.

      # This is a workaround to an apparent bug where logstash treats a non-existant field
      # as an array which causes a "Not possible to merge an array and a hash" error. Here
      # we force the probably non-existant field to a hash with a temp field so we can use
      # the mutate's merge option.
      mutate { add_field => [ "source[_forcemeahash]", "workaround" ] }

      mutate { merge => [ "source", "syslog_sd_params" ] }
      mutate { rename => [ "syslog_message", "message" ] }
      mutate { remove_field => [ "source[_forcemeahash]" ] }

      mutate {
        # syslog host is actually the shipper
        rename => [ "source.host", "shipper[host]" ]

        # these are parsed with kv, but they're static nxlog shipper properties
        rename => [ "source[EventReceivedTime]", "shipper[event_received_time]" ]
        rename => [ "source[SourceModuleName]", "shipper[module_name]" ]
        rename => [ "source[SourceModuleType]", "shipper[module_type]" ]
      }

      mutate { replace => [ "type", "%{type}_nxlog" ] }

      if [source][bosh_deployment] {
        mutate { add_field => { "[@metadata][index_name]" => "%{[source][bosh_deployment]}" } }
      }

    }
  }


  ###################################
  #          CLOUDFOUNDRY           #
  ###################################
  if [source.host] == "loggregator" {
    grok {
      match           => { "syslog_procid" => "\[(?<log_source>[^/\]]+)(?:/(?<log_source_id>[^\]]+))?\]" }
      tag_on_failure  => [ "fail/logsearch-for-cloudfoundry/loggregator/_grokparsefailure" ]
    }

    if !("fail/logsearch-for-cloudfoundry/loggregator/_grokparsefailure" in [tags]) {
      #If it looks like JSON, it must be JSON...
      if [syslog_message] =~ /^\s*{".*}\s*$/ {
        json { source => "syslog_message" }

        # todo seems like some messages have @timestamp in them? seems ci-specific
        date { match => [ "@timestamp", "ISO8601" ] }

      } else {
        mutate { add_field => [ "message", "%{syslog_message}" ] }

        if [message] == "-" {
          mutate { remove_field => "message" }
        }
      }

      mutate { rename => [ "syslog_program", "app_id" ] }

      mutate { replace => [ "type", "%{type}_cf" ] }

      mutate { add_field => { "[@metadata][index_name]" => "cloudfoundry" } }
    }


  } else if [syslog_program] =~ /vcap\..*/ {
    grok {
      match           => { "syslog_message" => "(?:\[job=%{NOTSPACE:job.name}|-) +(?:index=%{NOTSPACE:job.index}\]|-) %{GREEDYDATA:_message_json}" }
      tag_on_failure  => [ "_grokparsefailure-cf-vcap" ]
    }

    if !("_grokparsefailure-cf-vcap" in [tags]) {
      kv {
        source      => "msgdata"
        field_split => " "
        target      => "msgdata"
      }

      if [_message_json] =~ /^\s*{".*}\s*$/ {
        json {
          source        => "_message_json"
          remove_field  => "_message_json"
        }
      } else {
        mutate { rename => [ "_message_json", "message" ] }
      }


      mutate {
        rename  => [ "syslog_program", "shipper.name" ]
        replace => [ "job.host", "%{source.host}" ]
        gsub    => [
          "shipper.name", "\.", "_",
          "job.name", "\.", "_"
        ]
      }

      if [source] == "NatsStreamForwarder" {
        if [data][nats_message] =~ /^\s*{".*}\s*$/ {
          json {
            source => "[data][nats_message]"
            target => "nats_message"
          }

          if [nats_message][start] {
            date {
              match => [ "[nats_message][start]", "yyyy-MM-dd HH:mm:ss Z", "ISO8601" ]
              target => "[nats_message][start]"
            }
          }
        }
      }

      mutate {
        replace => [ "shipper.priority", "%{syslog_pri}" ]
        replace => [ "shipper.name", "%{shipper.name}_%{type}" ]
      }

      mutate { replace => [ "type", "%{type}_cf" ] }

      mutate { add_field => { "[@metadata][index_name]" => "cloudfoundry" } }
    }
  }

  mutate {
    remove_field => "[data]"
    remove_field => "[nats_message][app_id_to_count]"

    remove_field => "syslog_sd"
    remove_field => "syslog_sd_id"
    remove_field => "syslog_sd_params"
    remove_field => "syslog_sd_params_raw"
    remove_field => "syslog_program"
    remove_field => "syslog_pri"
    remove_field => "syslog_hostname"
    remove_field => "syslog_port"
    remove_field => "syslog_timestamp"
    remove_field => "syslog_facility"
    remove_field => "syslog_facility_code"
    remove_field => "syslog_message"
    remove_field => "syslog_severity"
    remove_field => "syslog_severity_code"
    remove_field => "syslog5424_ver"
    remove_field => "syslog6587_msglen"
    remove_field => "syslog_procid"
    remove_field => "syslog_msgid"
  }

  # remove fields with empty values
  ruby {
    code => "
      def cleanup(hsh)
        hsh.keep_if do |k, v|
          if v.is_a?(Hash)
            cleanup(v)
          else
            v != ''
          end
        end
      end
      cleanup(event.to_hash);
    "
  }

  # drop if not processed by our filters
  if ![@metadata][index_name] {
    drop { }
  }
}


output {
<% if p("logstash.elasticsearch.host", nil) %>
  if [@metadata][index_name] {
    elasticsearch {
      host            => "<%= p("logstash.elasticsearch.host") %>"
      cluster         => "<%= p("logstash.elasticsearch.cluster") %>"
      idle_flush_time => <%= p("logstash.elasticsearch.idle_flush_time") %>
      workers         => <%= p("logstash.elasticsearch.workers") %>
      protocol        => "transport"
      index           => "%{[@metadata][index_name]}-%{+YYYY.MM.dd}"
      manage_template => false
    }
  }
<% else %>
  stdout { codec => rubydebug { metadata => true } }
<% end %>
}
